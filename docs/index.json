[{"content":"In 2020, I was working at a company that offered a digital wallet service, used by over 3 million users. During my time there, my team and I spent months developing a feature eagerly anticipated by our users. When we finally launched it, our tech infrastructure couldn\u0026rsquo;t handle the massive surge in traffic\u0026hellip; and eventually collapsed.\nWe learned a lesson the hard way: \u0026ldquo;New features have to be released gradually\u0026rdquo;.\nIn this article, I\u0026rsquo;ll break down different strategies of what\u0026rsquo;s called Gradual Rollouts, which we used for the following features we launched. This is a phased approach in engineering for releasing updates that enables testing stability and performance with a limited audience, identifying issues with minimal overall user impact.\nDark Launching Dark Launching is a backend technique that involves introducing a new feature, which is then released to current users without their knowledge. This technique provides insights into how the feature would behave in production without actually impacting users.\nIn the dark launching process, we modify our systems to run the new feature in the background, without showing any changes to users. This involves executing all its functions, data processing and system interactions silently.\nMeanwhile, this allows test engineers to evaluate the new feature performance in a real-world setting, without impacting the user experience.\nThis process allows for performance testing, issue detection, infrastructure scaling analysis and stress testing under actual traffic conditions, while keeping it hidden from users.\nDark launching is particularly effective for critical updates where precision is crucial and risks are high. Its strength lies in testing features discreetly, maintaining user experience while identifying potential issues early. However, it requires significant backend effort and may not capture real user reactions or feedback, as users are unaware of the changes being tested.\nCanary Release Canary Release is a method used to minimize the risk of deploying new versions by initially introducing the update to a small group of users. This approach involves gradually extending the update across the entire system.\nThis technique starts with the new version being deployed to a part of the infrastructure not yet accessed by users, and then the process continues by gradually routing some user traffic to the new version.\nThe term is derived from an old mining practice, where miners took canaries into mines as an early warning system against toxic gases. Just as the canary would alert miners to danger, a canary release detects potential issues in a software update before it affects all users.\nIn modern distributed systems, instead of using a router for this process, various strategies are employed. These include rolling out updates to specific geographic regions first or to specific user groups based on criteria like user behavior, device type, or membership tier.\nCanary Release is particularly effective for applications requiring extensive user feedback where minimizing risk is crucial. The major advantage lies in its ability to detect issues early with minimal user impact. However, it demands meticulous monitoring and can lead to a slower overall release process.\nðŸ’¡ The concepts of Dark Launching and Canary Release have evolved over time, leading to some confusion. Nowadays, they are often used interchangeably, but it\u0026rsquo;s important to note that they are not the same. Alongside the mentioned strategies, Feature Toggles are essential in implementing Canary Releases. They enable the introduction of new features within the same production environment. This approach allows for incremental rollouts to these specific user groups, streamlining the release process and reducing the need for maintaining multiple environments.\nðŸ’¡ Feature Toggle is also referred to as Feature Flag in some engineering contexts, highlighting its role in enabling and disabling certain features. Shadow Testing Shadow testing is a technique used to compare the performance of a new feature within a shadow environment, a replica of the current production environment. The primary goal is to identify and mitigate potential risks associated with the new feature before it\u0026rsquo;s released to users, ensuring the process remains invisible to them.\nA shadow environment is created to precisely mimic real user traffic and interactions as they occur in the production environment. The new feature is deployed within this shadow environment, allowing it to operate alongside the production environment without affecting real users.\nTest engineers then analyze the behavior and responses of the new feature in the shadow environment in comparison with the production environment, to detect any discrepancies, performance issues, or risks. This thorough evaluation ensures that any potential problems are addressed before the new feature is officially introduced to the production environment.\nThis technique is particularly convenient in scenarios where accurate performance assessment under real-world conditions is essential, without the risk of disrupting user experience. Itâ€™s ideal for systems that handle sensitive data or complex transactions, where unnoticed errors could lead to significant issues.\nThe primary advantage of Shadow Testing is its ability to provide a realistic evaluation of new features, without affecting end users. However, it can be resource-intensive to set up and maintain a parallel environment, and it may not fully capture user experience aspects, since interactions are only simulated, not directly observed.\nTL;DR GradualÂ RolloutÂ Strategy Description Dark Launching Deploying a feature in production without making it visible to users. The functionality is dark to users, but can be selectively enabled for testing purposes. Canary Release Rolling out a new feature incrementally to a small subset of users before a full deployment, often implemented through the use of Feature Toggles. Shadow Testing Duplicating real traffic to a parallel (shadow) new environment version, which processes it without affecting or being noticed by users, allowing performance and stability testing under real conditions. Acknowledgements With deep gratitude, I would like to highlight the significant contributions of my great friends, NicolÃ¡s Cerdeira and Bautista Coronado, whose expertise was crucial in enriching this article, and their input went beyond mere feedback. I also offer heartfelt thanks to my dear former colleagues, Juan AgÃº and Nicolas Garay, for their valuable support and insights throughout the drafting process. \u0026ldquo;The Top 10 Adages in Continuous Deployment\u0026rdquo; by C. Parnin et al. (2017)\n\u0026ldquo;Weâ€™re Doing It Live\u0026rdquo; by G. Schermann, J. Cito, P. Leitner, U. Zdun and H. C. Gall (2018).\n","permalink":"https://lmitelman.github.io/posts/silent-launches/","summary":"In 2020, I was working at a company that offered a digital wallet service, used by over 3 million users. During my time there, my team and I spent months developing a feature eagerly anticipated by our users. When we finally launched it, our tech infrastructure couldn\u0026rsquo;t handle the massive surge in traffic\u0026hellip; and eventually collapsed.\nWe learned a lesson the hard way: \u0026ldquo;New features have to be released gradually\u0026rdquo;.","title":"The Art of Gradual Rollouts"},{"content":"A few weeks ago, one of the best engineers on my team made an important comment in a pull request:\n\u0026ldquo;Remember not to break the Tell-Don\u0026rsquo;t-Ask principle.\u0026rdquo;\nâ€” Fernando Balmaceda, the great.\nLet\u0026rsquo;s delve into this concept a bit\u0026hellip;\nIn object-oriented software, a typical use case involves executing logic based on an object\u0026rsquo;s internal state. For instance, consider a scenario where we want to sound an alarm if a thermometer reaches a certain temperature:\nclass Thermometer { private temperature: number = 0; increaseTemperature(value: number): void { this.temperature += value; } getTemperature(): number { return this.temperature; } }; class Alarm { sound(): void { console.log(\u0026#39;RIIING!\u0026#39;); } }; In this \u0026ldquo;asking\u0026rdquo; approach, to detect the temperature and sound an alarm, we would have to:\nconst thermometer = new Thermometer(); thermometer.increaseTemperature(35); const temperature = thermometer.getTemperature(); if (temperature \u0026gt; 30) { const alarm = new Alarm(); alarm.sound(); }; However, following the Tell-Don\u0026rsquo;t-Ask principle suggests that the logic should reside within the object itself if it is related to that object. In other words, the thermometer should be responsible for sounding the alarm when it reaches a certain temperature. This promotes more object-oriented code as opposed to procedural code.\nProcedural code gets information then makes decisions. Object-oriented code tells objects to do things.\nWith this in mind, let\u0026rsquo;s refactor our classes using a \u0026ldquo;telling\u0026rdquo; approach:\nclass Thermometer { private temperature: number = 0; private alarm: Alarm; constructor(alarm: Alarm) { this.alarm = alarm; } increaseTemperature(value: number): void { this.temperature += value; if (this.temperature \u0026gt; 30) { this.alarm.sound(); } } }; class Alarm { sound(): void { console.log(\u0026#39;RIIING!\u0026#39;); } }; Now, instead of checking the thermometer\u0026rsquo;s internal status to sound the alarm, we \u0026ldquo;tell\u0026rdquo; the thermometer to handle that:\nconst alarm = new Alarm(); const thermometer = new Thermometer(alarm); thermometer.increaseTemperature(35); To summarize, it is acceptable to \u0026ldquo;ask\u0026rdquo; for the state of an object and then execute certain logic based on that information. However, if that logic is inherently related to the object, it should be moved and become the responsibility of the object itself. By adhering to the Tell-Don\u0026rsquo;t-Ask principle, we can create more maintainable and object-oriented code.\nAcknowledgements I would like to extend my heartfelt thanks to Fernando Balmaceda, whose comment sparked the idea for this post. His contributions and mentorship have played a significant role in shaping my approach to software engineering. \u0026ldquo;The Art of Enbugging\u0026rdquo; by Andy Hunt and Dave Thomas (2003). ","permalink":"https://lmitelman.github.io/posts/tell-dont-ask/","summary":"A few weeks ago, one of the best engineers on my team made an important comment in a pull request:\n\u0026ldquo;Remember not to break the Tell-Don\u0026rsquo;t-Ask principle.\u0026rdquo;\nâ€” Fernando Balmaceda, the great.\nLet\u0026rsquo;s delve into this concept a bit\u0026hellip;\nIn object-oriented software, a typical use case involves executing logic based on an object\u0026rsquo;s internal state. For instance, consider a scenario where we want to sound an alarm if a thermometer reaches a certain temperature:","title":"Tell-Donâ€™t-Ask"}]